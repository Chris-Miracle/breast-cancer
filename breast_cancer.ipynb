{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1lFTp_HpmGHPrDALFe99p34NshOwyBuVU",
      "authorship_tag": "ABX9TyPYn/srkCUNDuiK5uwcoyka"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "kcLrEqirtkGn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Wgm2JaTXtTkJ"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing all necessary libraries to facilitate prediction objective.\n"
      ],
      "metadata": {
        "id": "b0hImz06uBs0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "EisT2BPEuDTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "os.chdir('/content/drive/MyDrive/Colab/Datasets/')\n",
        "df = pd.read_csv('Breast_Cancer.csv')\n",
        "\n",
        "# Remove any missing values if any\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Handle duplicates if any\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Encode Outcome Variable 'Status', Alive - 1, Dead - 0\n",
        "df = pd.get_dummies(df, columns=['Status'], drop_first=True)\n",
        "\n",
        "# Encode other Categorical Data\n",
        "df = pd.get_dummies(df, columns=['Race', 'Marital Status', 'T Stage ', 'N Stage', '6th Stage', 'differentiate', 'A Stage', 'Estrogen Status', 'Progesterone Status'], drop_first=True)\n",
        "\n",
        "# Find Grade values = 'anaplastic; Grade IV' and convert to 4\n",
        "df['Grade'] = df['Grade'].map(lambda x: 4 if x == ' anaplastic; Grade IV' else x)\n",
        "\n",
        "# Split Data set 80% Training and 20% Test\n",
        "X = df.drop('Status_Dead', axis=1)\n",
        "y = df['Status_Dead']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check sizes\n",
        "print(f'Training set: {X_train.shape}')\n",
        "print(f'Test set: {X_test.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNPps4MVuChq",
        "outputId": "b7e60843-22c8-4d82-bb49-4f03e151f976"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: (3218, 27)\n",
            "Test set: (805, 27)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Load dataset from drive\n",
        "- Drop any missing values\n",
        "- Drop duplicates\n",
        "- Encode target variable, Alive = 0, Dead = 1\n",
        "- Encode other categorical variables\n",
        "- Set status as target variable\n",
        "- Split dataset 80% training, 20% Test"
      ],
      "metadata": {
        "id": "Orl-sqc6xqOk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fitting the Model"
      ],
      "metadata": {
        "id": "UsFkIEqvyEB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement Logistic regression to train data\n",
        "model_log = LogisticRegression()\n",
        "model_log.fit(X_train, y_train)\n",
        "\n",
        "y_pred_log = model_log.predict(X_test)\n",
        "\n",
        "# Implement MLPClassifier to train data\n",
        "model_mlp = MLPClassifier(hidden_layer_sizes=(10, 5), max_iter=5000)\n",
        "model_mlp.fit(X_train, y_train)\n",
        "\n",
        "y_pred_mlp = model_mlp.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVg_xDlVyDq3",
        "outputId": "70bff477-cd76-444e-9028-1fbcb52d7ff8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Train dataset on logistic regression and predict on unseen data(Test). Model is not converging based on the warning above. Adding 'max_iter', so the model would run multiple times and reach convergence would help fix the warning\n",
        "- Train dataset on MLPClassifier and Predict on unseen data(Test), two hidden layers, with 10 and 5 neurons each and 5000 iteration limit."
      ],
      "metadata": {
        "id": "BdaxN5Y8ypNM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Metrics"
      ],
      "metadata": {
        "id": "HRu9O3o13vlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy, precision, recall & F1 score for logistics\n",
        "accuracy_log = accuracy_score(y_test, y_pred_log)\n",
        "precision_log = precision_score(y_test, y_pred_log)\n",
        "recall_log = recall_score(y_test, y_pred_log)\n",
        "f1_score_log = f1_score(y_test, y_pred_log)\n",
        "\n",
        "# Calculate accuracy, precision, recall & F1 score for neural network\n",
        "accuracy_mlp = accuracy_score(y_test, y_pred_mlp)\n",
        "precision_mlp = precision_score(y_test, y_pred_mlp)\n",
        "recall_mlp = recall_score(y_test, y_pred_mlp)\n",
        "f1_score_mlp = f1_score(y_test, y_pred_mlp)\n",
        "\n",
        "# Print Values for both MLP and Logistics\n",
        "print('Logistic Regression Metrics:')\n",
        "print(f'Accuracy: {accuracy_log:.2f}')\n",
        "print(f'Precision: {precision_log:.2f}')\n",
        "print(f'Recall: {recall_log:.2f}')\n",
        "print(f'F1 Score: {f1_score_log:.2f}')\n",
        "\n",
        "print('\\nNeural Network Metrics:')\n",
        "print(f'Accuracy: {accuracy_mlp:.2f}')\n",
        "print(f'Precision: {precision_mlp:.2f}')\n",
        "print(f'Recall: {recall_mlp:.2f}')\n",
        "print(f'F1 Score: {f1_score_mlp:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-kbOyAt31ZO",
        "outputId": "4b13e57b-1759-4076-f267-2df9327a9bc4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Metrics:\n",
            "Accuracy: 0.90\n",
            "Precision: 0.81\n",
            "Recall: 0.45\n",
            "F1 Score: 0.58\n",
            "\n",
            "Neural Network Metrics:\n",
            "Accuracy: 0.90\n",
            "Precision: 0.81\n",
            "Recall: 0.46\n",
            "F1 Score: 0.59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the metrics for each model, **Accuracy** for how often the model predicted correctly, **Precision** how often did the model actually catch true classes that are actually false, **Recall** for how many true catches were actually true and **F1 score** for the balance between Precision and Recall."
      ],
      "metadata": {
        "id": "grjTRhSm6WZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison and Analysis"
      ],
      "metadata": {
        "id": "46yYYmFI7T4C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both **Logistic Regression** and the **Neural Network** had the same **accuracy (90%)** and **precision (81%)**, meaning they were equally good at predicting positives correctly. However, the **Neural Network slightly outperformed** Logistic Regression in **recall (46% vs 45%)** and **F1 score (0.59 vs 0.58)**. This means it was a bit better at identifying actual positives. The F1 score balances precision and recall, so a higher F1 shows slightly better overall performance. While both models are close, the Neural Network edges out as the better model for this classification task."
      ],
      "metadata": {
        "id": "n_9aBuJe7YHv"
      }
    }
  ]
}